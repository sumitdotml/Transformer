{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a pen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 3112, 13]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(input_text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "token_embeddings = torch.randn(len(token_ids), 5) # going with 5 embedding dimensions for simplicity\n",
    "print(f\"token_embeddings.shape: {token_embeddings.shape}\")\n",
    "print(f\"token_embeddings[0]: {token_embeddings[0]}\")\n",
    "print(f\"token_embeddings[0].shape: {token_embeddings[0].shape}\")\n",
    "print(f\"token_embeddings[0].unsqueeze(0).shape: {token_embeddings[0].unsqueeze(0).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
       "        [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
       "        [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
       "        [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
       "        [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "token_embeddings = tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
    "        [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
    "        [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
    "        [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
    "        [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]])\n",
    "```\n",
    "\n",
    "writing the token embeddings tensor above in latex below:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 1.9269 & 1.4873 & 0.9007 & -2.1055 & 0.6784 \\\\ -1.2345 & -0.0431 & -1.6047 & -0.7521 & -0.6866 \\\\ -0.4934 & 0.2415 & -1.1109 & 0.0915 & -2.3169 \\\\ -0.2168 & -1.3847 & -0.3957 & 0.8034 & -0.6216 \\\\ -0.5920 & -0.0631 & -0.8286 & 0.3309 & -1.5576 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(seq_len, d, n=10000):\n",
    "    P = torch.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d):\n",
    "            denominator = torch.tensor(n).pow(-i/d)\n",
    "            P[k, i] = torch.sin(k * denominator) if i % 2 == 0 else torch.cos(k * denominator)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  2.4873,  0.9007, -1.1055,  0.6784],\n",
       "        [-0.3931,  0.9444, -1.5796,  0.2479, -0.6860],\n",
       "        [ 0.4159,  1.1917, -1.0607,  1.0915, -2.3157],\n",
       "        [-0.0757, -0.4956, -0.3204,  1.8033, -0.6197],\n",
       "        [-1.3488,  0.7426, -0.7282,  1.3308, -1.5550]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embed_w_pos_encoding = token_embeddings + pos_encoding(len(token_ids), token_embeddings.shape[1])\n",
    "token_embed_w_pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding Logic:\n",
    "\n",
    "The formula for the positional encoding is:\n",
    "\n",
    "$$\n",
    "P(pos, 2i) = \\sin\\left(\\frac{pos}{n^{2i/d}}\\right)\n",
    "$$\n",
    "$$\n",
    "P(pos, 2i+1) = \\cos\\left(\\frac{pos}{n^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "Here, the first $P(pos, 2i)$ is the sine function and the second $P(pos, 2i+1)$ is the cosine function.\n",
    "\n",
    "The logic here is that we use sine when the position is even and cosine when the position is odd.\n",
    "\n",
    "This is because the sine and cosine functions are orthogonal to each other, which allows us to represent any function as a sum of sine and cosine functions.\n",
    "\n",
    "token_embed_w_pos_encoding\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 1.9269 & 2.4873 & 0.9007 & -1.1055 & 0.6784 \\\\ -0.3931 & 0.9444 & -1.5796 & 0.2479 & -0.6860 \\\\ 0.4159 & 1.1917 & -1.0607 & 1.0915 & -2.3157 \\\\ -0.0757 & -0.4956 & -0.3204 & 1.8033 & -0.6197 \\\\ -1.3488 & 0.7426 & -0.7282 & 1.3308 & -1.5550 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_size, row_size=0, col_size=1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim_size, dim_size, bias=False)\n",
    "        self.query = nn.Linear(dim_size, dim_size, bias=False)\n",
    "        self.value = nn.Linear(dim_size, dim_size, bias=False)\n",
    "        self.row_size = row_size\n",
    "        self.col_size = col_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
