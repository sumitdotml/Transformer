# Transformer

Replicating the paper "Attention is All You Need"

---

My plan with going through the architecture of the Transformer is through different sub-modules of complexity and the level of detail I go into.

I first started with getting a basic understanding of the Transformer architecture in the [0-intro](./0-intro/README.md) directory.

Then, I went into the sub-modules of the Transformer as part of my deep dive marathon. Documented quite a bit of it in the [1-diving-deeper](./1-diving-deeper/README.md) directory.

At the moment, I have assembled everything I coded so far throughout my learning journey in the [transformer](./transformer/README.md) directory. Unlike the paper, I tried to change it to be more compatible with EN <-> JP translation.

I have pieced together different things comprehensively, so if you're just starting out with learning about these seminal inventions, maybe this is a good starting point.

I've laid out all the resources I used across directories 0 & 1 (the transformer directory is me just piecing it all together), so maybe you'll find them useful.

---

Happy learning!
