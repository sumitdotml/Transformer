{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a pen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 3112, 13]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(input_text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "token_embeddings = torch.randn(\n",
    "    len(token_ids), 5\n",
    ")  # going with 5 embedding dimensions for simplicity\n",
    "print(f\"token_embeddings.shape: {token_embeddings.shape}\")\n",
    "print(f\"token_embeddings[0]: {token_embeddings[0]}\")\n",
    "print(f\"token_embeddings[0].shape: {token_embeddings[0].shape}\")\n",
    "print(\n",
    "    f\"token_embeddings[0].unsqueeze(0).shape: {token_embeddings[0].unsqueeze(0).shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
       "        [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
       "        [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
       "        [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
       "        [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "token_embeddings = tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
    "        [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
    "        [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
    "        [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
    "        [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]])\n",
    "```\n",
    "\n",
    "writing the token embeddings tensor above in latex below:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 1.9269 & 1.4873 & 0.9007 & -2.1055 & 0.6784 \\\\ -1.2345 & -0.0431 & -1.6047 & -0.7521 & -0.6866 \\\\ -0.4934 & 0.2415 & -1.1109 & 0.0915 & -2.3169 \\\\ -0.2168 & -1.3847 & -0.3957 & 0.8034 & -0.6216 \\\\ -0.5920 & -0.0631 & -0.8286 & 0.3309 & -1.5576 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(seq_len, d, n=10000):\n",
    "    P = torch.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d):\n",
    "            denominator = torch.tensor(n).pow(-i / d)\n",
    "            P[k, i] = (\n",
    "                torch.sin(k * denominator) if i % 2 == 0 else torch.cos(k * denominator)\n",
    "            )\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  2.4873,  0.9007, -1.1055,  0.6784],\n",
       "        [-0.3931,  0.9444, -1.5796,  0.2479, -0.6860],\n",
       "        [ 0.4159,  1.1917, -1.0607,  1.0915, -2.3157],\n",
       "        [-0.0757, -0.4956, -0.3204,  1.8033, -0.6197],\n",
       "        [-1.3488,  0.7426, -0.7282,  1.3308, -1.5550]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embed_w_pos_encoding = token_embeddings + pos_encoding(\n",
    "    len(token_ids), token_embeddings.shape[1]\n",
    ")\n",
    "token_embed_w_pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding Logic:\n",
    "\n",
    "The formula for the positional encoding is:\n",
    "\n",
    "$$\n",
    "P(pos, 2i) = \\sin\\left(\\frac{pos}{n^{2i/d}}\\right)\n",
    "$$\n",
    "$$\n",
    "P(pos, 2i+1) = \\cos\\left(\\frac{pos}{n^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "Here, the first $P(pos, 2i)$ is the sine function and the second $P(pos, 2i+1)$ is the cosine function.\n",
    "\n",
    "The logic here is that we use sine when the position is even and cosine when the position is odd.\n",
    "\n",
    "This is because the sine and cosine functions are orthogonal to each other, which allows us to represent any function as a sum of sine and cosine functions.\n",
    "\n",
    "token_embed_w_pos_encoding\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 1.9269 & 2.4873 & 0.9007 & -1.1055 & 0.6784 \\\\ -0.3931 & 0.9444 & -1.5796 & 0.2479 & -0.6860 \\\\ 0.4159 & 1.1917 & -1.0607 & 1.0915 & -2.3157 \\\\ -0.0757 & -0.4956 & -0.3204 & 1.8033 & -0.6197 \\\\ -1.3488 & 0.7426 & -0.7282 & 1.3308 & -1.5550 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim=5, row_dim=0, col_dim=1):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_k = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_v = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "    def forward(self, embedded_tokens_with_pos_encoding):\n",
    "        k = self.W_k(embedded_tokens_with_pos_encoding)\n",
    "        q = self.W_q(embedded_tokens_with_pos_encoding)\n",
    "        v = self.W_v(embedded_tokens_with_pos_encoding)\n",
    "\n",
    "        similarity_scores = torch.matmul(\n",
    "            q, k.transpose(dim0=self.row_dim, dim1=self.col_dim)\n",
    "        )  # this is Q @ K^T\n",
    "        scaled_similarity_scores = similarity_scores / torch.tensor(\n",
    "            k.shape[self.col_dim] ** 0.5\n",
    "        )  # this is Q @ K^T / sqrt(d_k)\n",
    "        attn_percents = torch.softmax(\n",
    "            scaled_similarity_scores, dim=self.col_dim\n",
    "        )  # apply softmax to get attention percentages\n",
    "        self_attn_output = torch.matmul(\n",
    "            attn_percents, v\n",
    "        )  # finally, (Q @ K^T / sqrt(d_k)) @ V\n",
    "\n",
    "        return self_attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is the implementation of the self-attention mechanism.\n",
    "\n",
    "Basically the formula for the self-attention mechanism is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1062,  0.6807, -0.5714,  0.4980,  0.0897],\n",
       "        [ 0.4504,  0.7748, -0.6737,  0.6146, -0.1540],\n",
       "        [ 0.5067,  0.8794, -0.7135,  0.6200, -0.1473],\n",
       "        [ 0.3436,  0.8276, -0.6767,  0.6013, -0.0335],\n",
       "        [ 0.4741,  0.8283, -0.6880,  0.6027, -0.1471]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "self_attn = SelfAttention()\n",
    "self_attn(token_embed_w_pos_encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
