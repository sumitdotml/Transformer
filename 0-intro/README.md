# README

## References

The references used for this `0-intro` folder are mainly from:

- [Attention in Transformers by Joshua Starmer, a DeepLearningAI course](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/)

- [Sebastian Raschka's Self-Attention blog](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)

---

## Concepts Covered

Not in a very deep or rigorous manner, but I am able to write simple implementations of:

- Self-Attention
- Multi-Head Attention

Things that I need to work more on:

- Positional Encoding (not yet covered)
- Encoder-Decoder Architecture (did a very simple implementation, need to work more on it)
- Transformer Architecture (need to dive deeper)

---

[Go to 1-diving-deeper](../1-diving-deeper/README.md)
