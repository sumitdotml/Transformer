{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    ## The only change from SelfAttention and attention is that\n",
    "    ## now we expect 3 sets of encodings to be passed in...\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "## create matrices of token encodings\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim=0,\n",
    "                      col_dim=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Disclaimer\n",
    "\n",
    "In the standard self-attention mechanism, `Q`, `K`, and `V` are all derived from the same input. For example, in the `SelfAttention` class from `1-simplest_selfattn.ipynb`, the `forward` method takes a single input and generates `Q`, `K`, `V` from it using linear transformations. That's self-attention where the same $\\text{encoding source}^{1}$ is used for all three.\n",
    "\n",
    "$$\n",
    "\\text{encoding source} = \\text{input text} \\rightarrow \\text{embeddings} \\rightarrow \\text{positional encodings} \\quad \\text{-------- (1)}\n",
    "$$\n",
    "\n",
    "But in the current `Attention` class of this notebook, the `forward` method accepts three different encodings: `encodings_for_q`, `encodings_for_k`, `encodings_for_v` because this is for encoder-decoder attention, <span style=\"color:lightgreen; font-weight:bold; font-size:1em;\">where the queries come from the decoder and the keys/values come from the encoder.</span>\n",
    "\n",
    "For simplicity, this notebook uses the same tensor for all three encodings; however, in real cases, the encoder processes the input sequence and produces `K` and `V`, while the decoder generates `Q` based on its own inputs. So the three encodings would come from different sources.\n",
    "\n",
    "The usual simplified version of my steps above $\\text{(1)}$ result in a single set of embeddings with positional info. But in encoder-decoder attention, the decoder's `Q` is based on its own embeddings (possibly masked), while `K` and `V` are from the encoder's output. Hence, the three separate inputs allow the `Attention` class to handle cases where `Q`, `K`, `V` come from different sequences, unlike self-attention where they're the same.\n",
    "\n",
    "So the key point is that this `Attention` class is designed for cross-attention between encoder and decoder, not self-attention. The three separate encodings parameters enable flexibility in handling different sources for `Q`, `K`, `V`, which is essential for tasks like translation where the decoder needs to attend to the encoder's output.\n",
    "\n",
    "---\n",
    "\n",
    "### So, in summary\n",
    "\n",
    "The Attention class accepts three separate encodings to handle different transformer architecture scenarios:\n",
    "\n",
    "#### 1. Self-Attention (single sequence):\n",
    "\n",
    "- All three encodings (`Q`, `K`, `V`) come from the same source\n",
    "- Example: Encoder self-attention using the same positional-encoded embeddings\n",
    "\n",
    "```python\n",
    "attention(same_encodings, same_encodings, same_encodings)\n",
    "```\n",
    "\n",
    "#### 2. Cross-Attention (encoder-decoder):\n",
    "\n",
    "- `Q` comes from decoder's embeddings\n",
    "- `K/V` come from encoder's final output\n",
    "\n",
    "```python\n",
    "attention(decoder_embeddings, encoder_outputs, encoder_outputs)\n",
    "```\n",
    "\n",
    "#### 3. Hybrid Scenarios:\n",
    "\n",
    "- Could mix different sources (e.g., `Q` from one modality, `K/V` from another)\n",
    "\n",
    "<span style=\"color:lightblue; font-weight:bold; font-size:1em;\">Key differences from standard self-attention:</span>\n",
    "\n",
    "| <span style=\"color:lightblue\">Scenario</span> | <span style=\"color:lightblue\">Q Source</span> | <span style=\"color:lightblue\">K/V Source</span> | <span style=\"color:lightblue\">Code Example</span> |\n",
    "|-----------------|-------------------|-------------------|---------------------------------------|\n",
    "| Encoder Self | Encoder Embeddings| Encoder Embeddings| Attention(enc, enc, enc) |\n",
    "| Decoder Self | Decoder Embeddings| Decoder Embeddings| Attention(dec, dec, dec) |\n",
    "| Encoder-Decoder | Decoder Embeddings| Encoder Outputs | Attention(dec, enc_out, enc_out) |\n",
    "\n",
    "This implementation mirrors the original Transformer architecture where:\n",
    "- Encoder outputs become `K/V` for decoder cross-attention\n",
    "- Decoder inputs (with positional encoding) become `Q`\n",
    "- Separate encoding parameters allow flexible attention between different sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
