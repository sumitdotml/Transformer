{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: we have an input text of 6 words, and each word is represented by a 512 dimension vector.\n",
    "\n",
    "In this case, the $sequence = 6$ and $d_{model} = 512$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\text{Input matrix} (sequence, d_{model}) = (6, 512)$\n",
    "\n",
    "Let's assume that our words are:\n",
    "\n",
    "$\\text{A, B, C, D, E, F}$\n",
    "\n",
    "Then, the illustration of the input matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\text{Input matrix} = \\begin{bmatrix}\n",
    "\\text{A}, & \\text{B}, & \\text{C}, & \\text{D}, & \\text{E}, & \\text{F} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's assume that each word has the following 512 columns of numerical representation (basically acting like 512 dimensions):\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{word} = \\begin{bmatrix}\n",
    "W_{0},  & W_{1}, & W_{2}, & \\dots & W_{511} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that each $W_{i}$ here is a number.\n",
    "\n",
    "With this background, the input matrix (let's call it $W$) in an expanded form should look like this:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{A}_1, & \\text{A}_2, & \\dots, & \\text{A}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{B}_0, & \\text{B}_1, & \\text{B}_2, & \\dots, & \\text{B}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{C}_0, & \\text{C}_1, & \\text{C}_2, & \\dots, & \\text{C}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{D}_0, & \\text{D}_1, & \\text{D}_2, & \\dots, & \\text{D}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{E}_0, & \\text{E}_1, & \\text{E}_2, & \\dots, & \\text{E}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{F}_0, & \\text{F}_1, & \\text{F}_2, & \\dots, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape, as we can clearly see, is $(6, 512)$.\n",
    "\n",
    "Also, the transpose of this looks like this:\n",
    "\n",
    "$$\n",
    "W^\\text{T} = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{B}_0, & \\text{C}_0, & \\text{D}_0, & \\text{E}_0, & \\text{F}_0 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_1, & \\text{B}_1, & \\text{C}_1, & \\text{D}_1, & \\text{E}_1, & \\text{F}_1 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_2, & \\text{B}_2, & \\text{C}_2, & \\text{D}_2, & \\text{E}_2, & \\text{F}_2 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_3, & \\text{B}_3, & \\text{C}_3, & \\text{D}_3, & \\text{E}_3, & \\text{F}_3 \\end{bmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix} \\text{A}_{511}, & \\text{B}_{511}, & \\text{C}_{511}, & \\text{D}_{511}, & \\text{E}_{511}, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape of this is $(512, 6)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 512]),\n",
       " tensor([[-0.2367,  1.8109,  0.1966,  ..., -0.6320,  0.3352,  0.3928],\n",
       "         [ 0.0783,  0.5694, -0.6083,  ...,  0.3377,  0.9911, -1.0636],\n",
       "         [ 0.0525, -0.4094, -0.7481,  ...,  0.7475, -1.0518, -0.2402],\n",
       "         [-0.7422,  0.5986,  1.1324,  ...,  0.6658, -1.9029,  0.3874],\n",
       "         [ 0.3757, -0.0370,  0.0536,  ...,  1.3480, -0.3502,  1.4096],\n",
       "         [ 0.3379,  0.6135,  0.3060,  ..., -0.1643, -1.4237,  1.1242]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(40)\n",
    "input_matrix = torch.randn(6, 512)\n",
    "input_matrix.shape, input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 6]),\n",
       " tensor([[-0.2367,  0.0783,  0.0525, -0.7422,  0.3757,  0.3379],\n",
       "         [ 1.8109,  0.5694, -0.4094,  0.5986, -0.0370,  0.6135],\n",
       "         [ 0.1966, -0.6083, -0.7481,  1.1324,  0.0536,  0.3060],\n",
       "         ...,\n",
       "         [-0.6320,  0.3377,  0.7475,  0.6658,  1.3480, -0.1643],\n",
       "         [ 0.3352,  0.9911, -1.0518, -1.9029, -0.3502, -1.4237],\n",
       "         [ 0.3928, -1.0636, -0.2402,  0.3874,  1.4096,  1.1242]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.T.shape, input_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[503.2782,  30.9015, -25.3849,  13.0093, -29.7297,  20.1604],\n",
       "         [ 30.9015, 536.9385, -23.3817, -70.6838,  -8.7096, -19.2283],\n",
       "         [-25.3849, -23.3817, 484.9792,  24.6444, -29.3236,  -0.9737],\n",
       "         [ 13.0093, -70.6838,  24.6444, 538.0654, -25.3541,  -8.6208],\n",
       "         [-29.7297,  -8.7096, -29.3236, -25.3541, 453.2016,  -5.3324],\n",
       "         [ 20.1604, -19.2283,  -0.9737,  -8.6208,  -5.3324, 539.1165]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_dotproduct = input_matrix @ input_matrix.T\n",
    "transpose_dotproduct.shape, transpose_dotproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/84mpvnn15tlby6jt5y456k5c0000gn/T/ipykernel_25829/3060185599.py:2: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  A @ A.T # basically dot product of A and A^T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(503.2783)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = input_matrix[0][:] # first row\n",
    "A @ A.T # basically dot product of A and A^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Formulas for positional encoding in the original transformer paper are:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = sin(pos /  10000^{2i/d_{model}}) \\quad \\leftarrow \\text{(for even i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}}) \\quad \\leftarrow \\text{(for odd i)}\n",
    "$$\n",
    "\n",
    "where $pos$ is the position of the word in the sentence, and $i$ is the dimension index (starting from 0 to $d_{model} - 1$).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Formula for multi-head attention in the original transformer paper is:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^\\text{O}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$\n",
    "\n",
    "where $W_i^Q, W_i^K, W_i^V$ are the weights for the $i$-th head.\n",
    "\n",
    "and:\n",
    "\n",
    "$\n",
    "W^\\text{O} = \\begin{bmatrix}\n",
    "W_1^\\text{O}, & W_2^\\text{O}, & \\dots, & W_h^\\text{O}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab # 50257\n",
    "d_model = 512\n",
    "\n",
    "embedding_matrix = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    embeddings = embedding_matrix(torch.stack([torch.tensor(token_ids)], dim=0))\n",
    "    return token_ids, embeddings\n",
    "\n",
    "def decode_text(token_ids):\n",
    "    return tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'This is a keyboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 512])\n",
      "tensor([[[ 0.8884,  3.1415, -0.7241,  ..., -0.8155, -0.0804,  1.9023],\n",
      "         [ 0.6572, -1.5998,  1.3872,  ...,  0.3046,  0.6751, -1.5896],\n",
      "         [ 0.5262,  0.3106, -0.4733,  ...,  0.5922,  1.3402,  1.1434],\n",
      "         [-0.7888,  1.1974, -2.1260,  ..., -0.0547,  1.4225,  0.5274]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "[1212, 318, 257, 10586]\n"
     ]
    }
   ],
   "source": [
    "token_ids, input_embeddings = tokenize_text(input_text)\n",
    "\n",
    "# batch size, sequence length (i.e., number of words), embedding dimension\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a keyboard'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input embeddings -> DONE\n",
    "\n",
    "### Let's do some positional encoding now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos: int, i: int, d_model=d_model):\n",
    "    if i % 2 == 0:\n",
    "        return torch.sin(torch.tensor(pos) / torch.pow(torch.tensor(10000), (2*i) / d_model))\n",
    "    else:\n",
    "        return torch.cos(torch.tensor(pos) / torch.pow(torch.tensor(10000), (2*i) / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8884,  5.1415, -0.7241,  ...,  1.1845, -0.0804,  3.9023],\n",
      "         [ 2.3402, -0.4604,  2.9912,  ...,  2.3046,  0.6751,  0.4104],\n",
      "         [ 2.3448, -0.3912,  1.4430,  ...,  2.5922,  1.3402,  3.1434],\n",
      "         [-0.5066, -0.7416, -1.4404,  ...,  1.9453,  1.4225,  2.5274]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.8884,  4.1415, -0.7241,  ...,  0.1845, -0.0804,  2.9023],\n",
      "         [ 1.4987, -1.0301,  2.1892,  ...,  1.3046,  0.6751, -0.5896],\n",
      "         [ 1.4355, -0.0403,  0.4848,  ...,  1.5922,  1.3402,  2.1434],\n",
      "         [-0.6477,  0.2279, -1.7832,  ...,  0.9453,  1.4225,  1.5274]]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def encoded_embeddings(input_embeddings):\n",
    "    batch, seq_len, d_model = input_embeddings.shape\n",
    "    positions = torch.arange(seq_len, dtype=input_embeddings.dtype).unsqueeze(1)\n",
    "    dims = torch.arange(d_model, dtype=input_embeddings.dtype).unsqueeze(0)\n",
    "    denominator = torch.pow(torch.tensor(10000), (2 * dims) / d_model)\n",
    "    angles = positions / denominator\n",
    "    encoded = torch.empty_like(angles)\n",
    "    encoded[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "    encoded[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "    return encoded.unsqueeze(0).expand(batch, -1, -1) + input_embeddings\n",
    "\n",
    "print(encoded_embeddings(input_embeddings))\n",
    "print(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
