{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: we have an input text of 6 words, and each word is represented by a 512 dimension vector.\n",
    "\n",
    "In this case, the $sequence = 6$ and $d_{model} = 512$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\text{Input matrix} (sequence, d_{model}) = (6, 512)$\n",
    "\n",
    "Let's assume that our words are:\n",
    "\n",
    "$\\text{A, B, C, D, E, F}$\n",
    "\n",
    "Then, the illustration of the input matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\text{Input matrix} = \\begin{bmatrix}\n",
    "\\text{A}, & \\text{B}, & \\text{C}, & \\text{D}, & \\text{E}, & \\text{F} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's assume that each word has the following 512 columns of numerical representation (basically acting like 512 dimensions):\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{word} = \\begin{bmatrix}\n",
    "W_{0},  & W_{1}, & W_{2}, & \\dots & W_{511} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that each $W_{i}$ here is a number.\n",
    "\n",
    "With this background, the input matrix (let's call it $W$) in an expanded form should look like this:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{A}_1, & \\text{A}_2, & \\dots, & \\text{A}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{B}_0, & \\text{B}_1, & \\text{B}_2, & \\dots, & \\text{B}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{C}_0, & \\text{C}_1, & \\text{C}_2, & \\dots, & \\text{C}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{D}_0, & \\text{D}_1, & \\text{D}_2, & \\dots, & \\text{D}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{E}_0, & \\text{E}_1, & \\text{E}_2, & \\dots, & \\text{E}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{F}_0, & \\text{F}_1, & \\text{F}_2, & \\dots, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape, as we can clearly see, is $(6, 512)$.\n",
    "\n",
    "Also, the transpose of this looks like this:\n",
    "\n",
    "$$\n",
    "W^\\text{T} = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{B}_0, & \\text{C}_0, & \\text{D}_0, & \\text{E}_0, & \\text{F}_0 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_1, & \\text{B}_1, & \\text{C}_1, & \\text{D}_1, & \\text{E}_1, & \\text{F}_1 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_2, & \\text{B}_2, & \\text{C}_2, & \\text{D}_2, & \\text{E}_2, & \\text{F}_2 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_3, & \\text{B}_3, & \\text{C}_3, & \\text{D}_3, & \\text{E}_3, & \\text{F}_3 \\end{bmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix} \\text{A}_{511}, & \\text{B}_{511}, & \\text{C}_{511}, & \\text{D}_{511}, & \\text{E}_{511}, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape of this is $(512, 6)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 512]),\n",
       " tensor([[-0.2367,  1.8109,  0.1966,  ..., -0.6320,  0.3352,  0.3928],\n",
       "         [ 0.0783,  0.5694, -0.6083,  ...,  0.3377,  0.9911, -1.0636],\n",
       "         [ 0.0525, -0.4094, -0.7481,  ...,  0.7475, -1.0518, -0.2402],\n",
       "         [-0.7422,  0.5986,  1.1324,  ...,  0.6658, -1.9029,  0.3874],\n",
       "         [ 0.3757, -0.0370,  0.0536,  ...,  1.3480, -0.3502,  1.4096],\n",
       "         [ 0.3379,  0.6135,  0.3060,  ..., -0.1643, -1.4237,  1.1242]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(40)\n",
    "input_matrix = torch.randn(6, 512)\n",
    "input_matrix.shape, input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 6]),\n",
       " tensor([[-0.2367,  0.0783,  0.0525, -0.7422,  0.3757,  0.3379],\n",
       "         [ 1.8109,  0.5694, -0.4094,  0.5986, -0.0370,  0.6135],\n",
       "         [ 0.1966, -0.6083, -0.7481,  1.1324,  0.0536,  0.3060],\n",
       "         ...,\n",
       "         [-0.6320,  0.3377,  0.7475,  0.6658,  1.3480, -0.1643],\n",
       "         [ 0.3352,  0.9911, -1.0518, -1.9029, -0.3502, -1.4237],\n",
       "         [ 0.3928, -1.0636, -0.2402,  0.3874,  1.4096,  1.1242]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.T.shape, input_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[503.2782,  30.9015, -25.3849,  13.0093, -29.7297,  20.1604],\n",
       "         [ 30.9015, 536.9385, -23.3817, -70.6838,  -8.7096, -19.2283],\n",
       "         [-25.3849, -23.3817, 484.9792,  24.6444, -29.3236,  -0.9737],\n",
       "         [ 13.0093, -70.6838,  24.6444, 538.0654, -25.3541,  -8.6208],\n",
       "         [-29.7297,  -8.7096, -29.3236, -25.3541, 453.2016,  -5.3324],\n",
       "         [ 20.1604, -19.2283,  -0.9737,  -8.6208,  -5.3324, 539.1165]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_dotproduct = input_matrix @ input_matrix.T\n",
    "transpose_dotproduct.shape, transpose_dotproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/84mpvnn15tlby6jt5y456k5c0000gn/T/ipykernel_49899/3060185599.py:2: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  A @ A.T # basically dot product of A and A^T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(503.2783)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = input_matrix[0][:] # first row\n",
    "A @ A.T # basically dot product of A and A^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Formulas for positional encoding in the original transformer paper are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\quad \\text{(even indices)} \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\quad \\text{(odd indices)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $pos$ is the position of the word in the sentence, and $i$ is the dimension index (starting from 0 to $d_{model} - 1$).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Formula for multi-head attention in the original transformer paper is:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^\\text{O}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$\n",
    "\n",
    "where $W_i^Q, W_i^K, W_i^V$ are the weights for the $i$-th head.\n",
    "\n",
    "and:\n",
    "\n",
    "$\n",
    "W^\\text{O} = \\begin{bmatrix}\n",
    "W_1^\\text{O}, & W_2^\\text{O}, & \\dots, & W_h^\\text{O}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab # 50257\n",
    "d_model = 512\n",
    "\n",
    "embedding_matrix = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    embeddings = embedding_matrix(torch.stack([torch.tensor(token_ids)], dim=0))\n",
    "    return token_ids, embeddings\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    token_ids_list = [tokenizer.encode(text) for text in texts]\n",
    "\n",
    "    # need the max text size out of all the texts\n",
    "    max_seq_len = max([len(token_ids) for token_ids in token_ids_list])\n",
    "\n",
    "    # pad all the texts to the max sequence length (for batch processing)\n",
    "    padded_token_ids = []\n",
    "    for ids in token_ids_list:\n",
    "        padded_ids = ids + [0] * (max_seq_len - len(ids))  # Padding with 0s\n",
    "        padded_token_ids.append(padded_ids)\n",
    "\n",
    "    # Convert to tensor and get embeddings\n",
    "    tokens_tensor = torch.tensor(padded_token_ids)\n",
    "    embeddings = embedding_matrix(tokens_tensor)\n",
    "\n",
    "    return token_ids_list, embeddings\n",
    "\n",
    "def decode_text(token_ids):\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "def decode_batch(token_ids_list):\n",
    "    return [decode_text(token_ids) for token_ids in token_ids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "input_text1 = 'This is a keyboard'\n",
    "input_text2 = 'A nice coffee cup'\n",
    "input_text.append(input_text1)\n",
    "input_text.append(input_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For a single text:\n",
      "torch.Size([1, 4, 512])\n",
      "tensor([[[ 0.0444,  1.0484,  1.3557,  ..., -0.8322,  0.7427, -1.7000],\n",
      "         [ 1.0631, -0.8056, -3.3340,  ..., -0.2969,  0.5532, -0.0959],\n",
      "         [ 0.3199,  0.8923, -0.9175,  ..., -0.0884,  1.0423, -0.9149],\n",
      "         [ 0.6791, -1.2112,  3.0816,  ..., -0.1510, -1.8448,  0.3131]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "[1212, 318, 257, 10586]\n",
      "\n",
      "For batched processing:\n",
      "torch.Size([2, 4, 512])\n",
      "tensor([[[ 0.0444,  1.0484,  1.3557,  ..., -0.8322,  0.7427, -1.7000],\n",
      "         [ 1.0631, -0.8056, -3.3340,  ..., -0.2969,  0.5532, -0.0959],\n",
      "         [ 0.3199,  0.8923, -0.9175,  ..., -0.0884,  1.0423, -0.9149],\n",
      "         [ 0.6791, -1.2112,  3.0816,  ..., -0.1510, -1.8448,  0.3131]],\n",
      "\n",
      "        [[-0.1444, -0.6360, -0.8506,  ..., -0.4992, -0.8969, -0.5498],\n",
      "         [-0.4954, -0.9754,  0.8146,  ...,  1.6291, -0.8691,  1.2290],\n",
      "         [-1.2850,  0.4737, -0.5718,  ..., -1.0871, -1.0541, -1.4151],\n",
      "         [-0.6142, -1.2674,  0.0843,  ..., -1.4956, -1.5690, -0.5409]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "[[1212, 318, 257, 10586], [32, 3621, 6891, 6508]]\n"
     ]
    }
   ],
   "source": [
    "# for a single text\n",
    "token_ids_single, input_embeddings_single = tokenize_text(input_text[0])\n",
    "print(\"\\nFor a single text:\")\n",
    "print(input_embeddings_single.shape)\n",
    "print(input_embeddings_single)\n",
    "print(token_ids_single)\n",
    "\n",
    "# for batched processing\n",
    "token_ids, input_embeddings = tokenize_batch(input_text)\n",
    "\n",
    "# batch size, sequence length (i.e., number of words), embedding dimension\n",
    "print(\"\\nFor batched processing:\")\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded texts:\n",
      "This is a keyboard\n",
      "['This is a keyboard', 'A nice coffee cup']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDecoded texts:\")\n",
    "print(decode_text(token_ids_single))\n",
    "print(decode_batch(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input embeddings -> DONE\n",
    "\n",
    "### Let's do some positional encoding now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_simple(pos: int, i: int, d_model=d_model):\n",
    "    if i % 2 == 0:\n",
    "        return torch.sin(torch.tensor(pos) / torch.pow(torch.tensor(10000), (2*i) / d_model))\n",
    "    else:\n",
    "        return torch.cos(torch.tensor(pos) / torch.pow(torch.tensor(10000), (2*i) / d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was a very simple function to understand the positional encoding. However, this helps calculate the positional encoding for a single position and a single dimension. Of course, I can always expand this, but instead of a loop, I think it's better to use vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0444,  1.0484,  1.3557,  ..., -0.8322,  0.7427, -1.7000],\n",
      "         [ 1.0631, -0.8056, -3.3340,  ..., -0.2969,  0.5532, -0.0959],\n",
      "         [ 0.3199,  0.8923, -0.9175,  ..., -0.0884,  1.0423, -0.9149],\n",
      "         [ 0.6791, -1.2112,  3.0816,  ..., -0.1510, -1.8448,  0.3131]],\n",
      "\n",
      "        [[-0.1444, -0.6360, -0.8506,  ..., -0.4992, -0.8969, -0.5498],\n",
      "         [-0.4954, -0.9754,  0.8146,  ...,  1.6291, -0.8691,  1.2290],\n",
      "         [-1.2850,  0.4737, -0.5718,  ..., -1.0871, -1.0541, -1.4151],\n",
      "         [-0.6142, -1.2674,  0.0843,  ..., -1.4956, -1.5690, -0.5409]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[ 0.0444,  2.0484,  1.3557,  ...,  0.1678,  0.7427, -0.7000],\n",
      "         [ 1.9046, -0.2359, -2.5320,  ...,  0.7031,  0.5532,  0.9041],\n",
      "         [ 1.2292,  0.5414,  0.0406,  ...,  0.9116,  1.0423,  0.0851],\n",
      "         [ 0.8202, -2.1807,  3.4244,  ...,  0.8490, -1.8448,  1.3131]],\n",
      "\n",
      "        [[-0.1444,  0.3640, -0.8506,  ...,  0.5008, -0.8969,  0.4502],\n",
      "         [ 0.3461, -0.4057,  1.6166,  ...,  2.6291, -0.8691,  2.2290],\n",
      "         [-0.3757,  0.1228,  0.3864,  ..., -0.0871, -1.0541, -0.4151],\n",
      "         [-0.4731, -2.2369,  0.4271,  ..., -0.4956, -1.5690,  0.4591]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# vectorized and scalable version\n",
    "\n",
    "def positional_encoding_original_paper(input_embeddings):\n",
    "    batch, seq_len, d_model = input_embeddings.shape\n",
    "    device = input_embeddings.device\n",
    "    \n",
    "    # Create position and dimension indices\n",
    "    pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    dim = torch.arange(d_model, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Compute angle rates\n",
    "    angle_rates = pos / (10000 ** (2*dim/d_model))\n",
    "    \n",
    "    # Initialize encoding matrix\n",
    "    pe = torch.zeros(seq_len, d_model, device=device)\n",
    "    \n",
    "    # Apply sine to even indices, cosine to odd indices\n",
    "    pe[:, 0::2] = torch.sin(angle_rates[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angle_rates[:, 1::2])\n",
    "    \n",
    "    # Add batch dimension and return\n",
    "    return pe.unsqueeze(0).expand(batch, -1, -1) + input_embeddings\n",
    "\n",
    "final_embeddings = positional_encoding_original_paper(input_embeddings)\n",
    "\n",
    "print(input_embeddings)\n",
    "print(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0484, grad_fn=<AddBackward0>)\n",
      "tensor(2.0484, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(positional_encoding_simple(0, 1) + input_embeddings[0][0][1])\n",
    "print(positional_encoding_original_paper(input_embeddings)[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more scalable version for the positional encoding is to use the following formula:\n",
    "\n",
    "$$\n",
    "PE_{(pos, i)} = \\begin{cases} \n",
    "\\sin\\left(pos \\cdot \\exp\\left(-\\frac{2i}{d_{\\text{model}}} \\ln (10000)\\right)\\right) & \\text{if } i \\text{ even} \\\\\n",
    "\\cos\\left(pos \\cdot \\exp\\left(-\\frac{2i}{d_{\\text{model}}} \\ln (10000)\\right)\\right) & \\text{if } i \\text{ odd}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "compared to the original paper's formula:\n",
    "\n",
    "$$\n",
    "PE_{(pos, i)} = \\begin{cases} \n",
    "\\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) & \\text{if } i \\text{ even} \\\\\n",
    "\\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) & \\text{if } i \\text{ odd}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Proof of Equivalence:\n",
    "$$\n",
    "\\frac{pos}{10000^{2i/d_{\\text{model}}}} = pos \\cdot \\exp\\left(-\\frac{2i}{d_{\\text{model}}} \\ln (10000)\\right)\n",
    "$$\n",
    " \n",
    "This holds because:\n",
    "\n",
    "$$\n",
    "10000^x = e^{x \\ln (10000)} \\Rightarrow \\frac{1}{10000} = e^{-\\ln (10000)}\n",
    "$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0444,  2.0484,  1.3557,  ...,  0.1678,  0.7427, -0.7000],\n",
       "         [ 1.9046, -0.2359, -2.5320,  ...,  0.7031,  0.5532,  0.9041],\n",
       "         [ 1.2292,  0.5414,  0.0406,  ...,  0.9116,  1.0423,  0.0851],\n",
       "         [ 0.8202, -2.1807,  3.4244,  ...,  0.8490, -1.8448,  1.3131]],\n",
       "\n",
       "        [[-0.1444,  0.3640, -0.8506,  ...,  0.5008, -0.8969,  0.4502],\n",
       "         [ 0.3461, -0.4057,  1.6166,  ...,  2.6291, -0.8691,  2.2290],\n",
       "         [-0.3757,  0.1228,  0.3864,  ..., -0.0871, -1.0541, -0.4151],\n",
       "         [-0.4731, -2.2369,  0.4271,  ..., -0.4956, -1.5690,  0.4591]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    batch, seq_len, d_model = input_embeddings.shape\n",
    "    pos = torch.arange(seq_len).unsqueeze(1)\n",
    "    dim = torch.arange(d_model)\n",
    "\n",
    "    angle_rates = pos * torch.exp((-2 * dim * torch.log(torch.tensor(10000))) / d_model)\n",
    "\n",
    "    pe = torch.zeros(seq_len, d_model) # positional encoding initialized\n",
    "    pe[:, 0::2] = torch.sin(angle_rates[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angle_rates[:, 1::2])\n",
    "    \n",
    "    # add batch dimension\n",
    "    pe = pe.unsqueeze(0).expand(batch, -1, -1) + input_embeddings\n",
    "    return pe\n",
    "\n",
    "positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded embedding using simple function: 2.0483908653259277\n",
      "Encoded embedding using original paper's formula: 2.0483908653259277\n",
      "Encoded embedding using the vectorized and scalable version: 2.0483908653259277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison\n",
    "\n",
    "print(f\"Encoded embedding using simple function: {positional_encoding_simple(0, 1) + input_embeddings[0][0][1]}\")\n",
    "print(f\"Encoded embedding using original paper's formula: {positional_encoding_original_paper(input_embeddings)[0][0][1]}\")\n",
    "print(f\"Encoded embedding using the vectorized and scalable version: {positional_encoding(input_embeddings)[0][0][1]}\")\n",
    "\n",
    "positional_encoding(input_embeddings)[0][0][1] == positional_encoding_simple(0, 1) + input_embeddings[0][0][1] == positional_encoding_original_paper(input_embeddings)[0][0][1] == positional_encoding(input_embeddings)[0][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is the same!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
