{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: we have an input text of 6 words, and each word is represented by a 512 dimension vector.\n",
    "\n",
    "In this case, the $sequence = 6$ and $d_{model} = 512$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\text{Input matrix} (sequence, d_{model}) = (6, 512)$\n",
    "\n",
    "Let's assume that our words are:\n",
    "\n",
    "$\\text{A, B, C, D, E, F}$\n",
    "\n",
    "Then, the illustration of the input matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\text{Input matrix} = \\begin{bmatrix}\n",
    "\\text{A}, & \\text{B}, & \\text{C}, & \\text{D}, & \\text{E}, & \\text{F} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's assume that each word has the following 512 columns of numerical representation (basically acting like 512 dimensions):\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{word} = \\begin{bmatrix}\n",
    "W_{0},  & W_{1}, & W_{2}, & \\dots & W_{511} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that each $W_{i}$ here is a number.\n",
    "\n",
    "With this background, the input matrix (let's call it $W$) in an expanded form should look like this:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{A}_1, & \\text{A}_2, & \\dots, & \\text{A}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{B}_0, & \\text{B}_1, & \\text{B}_2, & \\dots, & \\text{B}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{C}_0, & \\text{C}_1, & \\text{C}_2, & \\dots, & \\text{C}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{D}_0, & \\text{D}_1, & \\text{D}_2, & \\dots, & \\text{D}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{E}_0, & \\text{E}_1, & \\text{E}_2, & \\dots, & \\text{E}_{511} \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{F}_0, & \\text{F}_1, & \\text{F}_2, & \\dots, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape, as we can clearly see, is $(6, 512)$.\n",
    "\n",
    "Also, the transpose of this looks like this:\n",
    "\n",
    "$$\n",
    "W^\\text{T} = \\begin{bmatrix}\n",
    "\\begin{bmatrix} \\text{A}_0, & \\text{B}_0, & \\text{C}_0, & \\text{D}_0, & \\text{E}_0, & \\text{F}_0 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_1, & \\text{B}_1, & \\text{C}_1, & \\text{D}_1, & \\text{E}_1, & \\text{F}_1 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_2, & \\text{B}_2, & \\text{C}_2, & \\text{D}_2, & \\text{E}_2, & \\text{F}_2 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} \\text{A}_3, & \\text{B}_3, & \\text{C}_3, & \\text{D}_3, & \\text{E}_3, & \\text{F}_3 \\end{bmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix} \\text{A}_{511}, & \\text{B}_{511}, & \\text{C}_{511}, & \\text{D}_{511}, & \\text{E}_{511}, & \\text{F}_{511} \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The shape of this is $(512, 6)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 512]),\n",
       " tensor([[-0.2367,  1.8109,  0.1966,  ..., -0.6320,  0.3352,  0.3928],\n",
       "         [ 0.0783,  0.5694, -0.6083,  ...,  0.3377,  0.9911, -1.0636],\n",
       "         [ 0.0525, -0.4094, -0.7481,  ...,  0.7475, -1.0518, -0.2402],\n",
       "         [-0.7422,  0.5986,  1.1324,  ...,  0.6658, -1.9029,  0.3874],\n",
       "         [ 0.3757, -0.0370,  0.0536,  ...,  1.3480, -0.3502,  1.4096],\n",
       "         [ 0.3379,  0.6135,  0.3060,  ..., -0.1643, -1.4237,  1.1242]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(40)\n",
    "input_matrix = torch.randn(6, 512)\n",
    "input_matrix.shape, input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 6]),\n",
       " tensor([[-0.2367,  0.0783,  0.0525, -0.7422,  0.3757,  0.3379],\n",
       "         [ 1.8109,  0.5694, -0.4094,  0.5986, -0.0370,  0.6135],\n",
       "         [ 0.1966, -0.6083, -0.7481,  1.1324,  0.0536,  0.3060],\n",
       "         ...,\n",
       "         [-0.6320,  0.3377,  0.7475,  0.6658,  1.3480, -0.1643],\n",
       "         [ 0.3352,  0.9911, -1.0518, -1.9029, -0.3502, -1.4237],\n",
       "         [ 0.3928, -1.0636, -0.2402,  0.3874,  1.4096,  1.1242]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.T.shape, input_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[503.2782,  30.9015, -25.3849,  13.0093, -29.7297,  20.1604],\n",
       "         [ 30.9015, 536.9385, -23.3817, -70.6838,  -8.7096, -19.2283],\n",
       "         [-25.3849, -23.3817, 484.9792,  24.6444, -29.3236,  -0.9737],\n",
       "         [ 13.0093, -70.6838,  24.6444, 538.0654, -25.3541,  -8.6208],\n",
       "         [-29.7297,  -8.7096, -29.3236, -25.3541, 453.2016,  -5.3324],\n",
       "         [ 20.1604, -19.2283,  -0.9737,  -8.6208,  -5.3324, 539.1165]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_dotproduct = input_matrix @ input_matrix.T\n",
    "transpose_dotproduct.shape, transpose_dotproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(503.2783)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = input_matrix[0][:] # first row\n",
    "A @ A.T # basically dot product of A and A^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Formulas for positional encoding in the original transformer paper are:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = sin(pos /  10000^{2i/d_{model}}) \\quad \\leftarrow \\text{(for even i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}}) \\quad \\leftarrow \\text{(for odd i)}\n",
    "$$\n",
    "\n",
    "where $pos$ is the position of the word in the sentence, and $i$ is the dimension index (starting from 0 to $d_{model} - 1$).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Formula for multi-head attention in the original transformer paper is:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^\\text{O}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$\n",
    "\n",
    "where $W_i^Q, W_i^K, W_i^V$ are the weights for the $i$-th head.\n",
    "\n",
    "and:\n",
    "\n",
    "$\n",
    "W^\\text{O} = \\begin{bmatrix}\n",
    "W_1^\\text{O}, & W_2^\\text{O}, & \\dots, & W_h^\\text{O}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
